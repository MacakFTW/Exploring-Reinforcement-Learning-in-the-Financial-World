{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6jVbFsHTrgu"
      },
      "source": [
        "# Step 1: Install dependencies:\n",
        "Installs the required libraries for data handling, plotting, and (deep) reinforcement learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "iRoemgLWTdgH"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance pandas numpy matplotlib stable-baselines3[extra] shimmy>=2.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Mount Google Drive:\n",
        "Mounts Google Drive to save and load files directly from the Colab session"
      ],
      "metadata": {
        "id": "0R_L_z7GCQQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "VVtWopN7vXDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ81fHVgVPXT"
      },
      "source": [
        "# Step 3: Import libraries:\n",
        "Imports necessary packages for (deep) reinforcement learning, data processing, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "An9RQ5qe5XUe"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Custom train/test split:\n",
        "Splits DataFrame into training and test sets based on the defined ratio."
      ],
      "metadata": {
        "id": "Z740kqtoCyWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xF50GqT4Dx3o"
      },
      "outputs": [],
      "source": [
        "def train_test_split(df, train_ratio=0.8):\n",
        "    train_size = int(len(df) * train_ratio)\n",
        "    return df[:train_size].copy(), df[train_size:].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkMEUl9TpIy"
      },
      "source": [
        "# Step 5: Custom Trading Environment:\n",
        "Defines the custom Gym environment that simulates stock trading with buy/sell hold actions and a realistic reward structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xNrzGxbKegYg"
      },
      "outputs": [],
      "source": [
        "class SimpleTradingEnv(gym.Env):\n",
        "    def __init__(self, df, lookback=5):\n",
        "        super(SimpleTradingEnv, self).__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.lookback = lookback\n",
        "        self.n_steps = len(self.df)\n",
        "        self.current_step = self.lookback  # start after enough history for lookback window\n",
        "        self.cash = 10000\n",
        "        self.stock_owned = 0\n",
        "        self.initial_cash = self.cash\n",
        "\n",
        "        # Observation: 4 OHLC + 2 SMA features per day √ó lookback days + cash + stock_owned\n",
        "        self.state_size = (4 + 2) * self.lookback + 2\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)  # action: -1 (sell all) to 1 (buy max)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.state_size,), dtype=np.float32)\n",
        "        self.trade_penalty_factor = 0.01  # penalty per share traded to discourage overtrading\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Get OHLC and SMA values over lookback window\n",
        "        frame = self.df.iloc[self.current_step - self.lookback:self.current_step]\n",
        "        ohlc = frame[[\"Open\", \"High\", \"Low\", \"Close\"]].values.flatten()\n",
        "        sma5 = frame[\"SMA5\"].values.flatten()\n",
        "        sma20 = frame[\"SMA20\"].values.flatten()\n",
        "        # Combine all features into a single observation vector\n",
        "        obs = np.concatenate([ohlc, sma5, sma20, [self.cash], [self.stock_owned]])\n",
        "        return obs.astype(np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset environment to initial state\n",
        "        self.current_step = self.lookback\n",
        "        self.cash = self.initial_cash\n",
        "        self.stock_owned = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Get current price and portfolio value before action\n",
        "        row = self.df.iloc[self.current_step]\n",
        "        price = float(row[\"Close\"])\n",
        "        prev_total_asset = self.cash + self.stock_owned * price\n",
        "\n",
        "        # Clip action to [-1, 1] and extract scala\n",
        "        action = float(action[0])\n",
        "        action = np.clip(action, -1, 1)\n",
        "\n",
        "        min_trade_amount = 1\n",
        "        shares_traded = 0\n",
        "\n",
        "        # Buy logic\n",
        "        if action > 0:\n",
        "            max_buy = int(self.cash // price) # max shares we can afford\n",
        "            shares_bought = int(action * max_buy) # fraction of max\n",
        "            if shares_bought >= min_trade_amount:\n",
        "                self.cash -= shares_bought * price\n",
        "                self.stock_owned += shares_bought\n",
        "                shares_traded = shares_bought\n",
        "\n",
        "        # Sell logic\n",
        "        elif action < 0:\n",
        "            shares_sold = int(abs(action) * self.stock_owned) # fraction of owned shares\n",
        "            if shares_sold >= min_trade_amount:\n",
        "                self.cash += shares_sold * price\n",
        "                self.stock_owned -= shares_sold\n",
        "                shares_traded = shares_sold\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.n_steps - 1\n",
        "        next_obs = self._get_obs()\n",
        "\n",
        "        # Get new portfolio value after action\n",
        "        next_price = float(self.df.iloc[self.current_step][\"Close\"]) if not done else price\n",
        "        new_total_asset = self.cash + self.stock_owned * next_price\n",
        "\n",
        "        # Reward: portfolio change minus penalty for trading too much\n",
        "        reward = new_total_asset - prev_total_asset - self.trade_penalty_factor * shares_traded\n",
        "\n",
        "        return next_obs, reward, done, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q3gRDfu09EU"
      },
      "source": [
        "# Step 6: Download, preprocess, train and evaluate:\n",
        "Downloads historical data for each Dow 30 stock, computes indicators, trains PPO & A2C agents, evaluates them, and compares to Buy & Hold, saves them to csv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uo4IiqmaVsB6"
      },
      "outputs": [],
      "source": [
        "# List of Dow Jones 30 tickers\n",
        "dow30_tickers = [\n",
        "    \"AAPL\", \"AMGN\", \"AXP\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"DIS\", \"DOW\", \"GS\",\n",
        "    \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"KO\", \"MCD\", \"MMM\", \"MRK\",\n",
        "    \"MSFT\", \"NKE\", \"PG\", \"CRM\", \"TRV\", \"UNH\", \"V\", \"VZ\", \"WBA\", \"WMT\"\n",
        "]\n",
        "\n",
        "# Dictionary to store results per stock\n",
        "results = {}\n",
        "\n",
        "# Loop through each stock ticker\n",
        "for ticker in dow30_tickers:\n",
        "    print(f\"\\nüìà Processing: {ticker}\")\n",
        "\n",
        "    try:\n",
        "        # Download historical data for the stock\n",
        "        raw = yf.download(ticker, start=\"2018-01-01\", end=\"2025-01-01\", group_by=None)\n",
        "\n",
        "        # Handle multi-index columns (if present)\n",
        "        if isinstance(raw.columns, pd.MultiIndex):\n",
        "            raw.columns = [col[1] if isinstance(col, tuple) else col for col in raw.columns]\n",
        "\n",
        "        # Select OHLC data and clean\n",
        "        df = raw[[\"Open\", \"High\", \"Low\", \"Close\"]].dropna().reset_index(drop=True)\n",
        "        df = df.astype(float)\n",
        "\n",
        "        # Add technical indicators: SMA5 and SMA20\n",
        "        df[\"SMA5\"] = df[\"Close\"].rolling(window=5).mean()\n",
        "        df[\"SMA20\"] = df[\"Close\"].rolling(window=20).mean()\n",
        "        df = df.dropna().reset_index(drop=True)  # drop initial rows without SMA\n",
        "\n",
        "        # Split into train/test sets chronologically\n",
        "        train_df, test_df = train_test_split(df, train_ratio=0.8)\n",
        "\n",
        "        # Create training and test environments\n",
        "        train_env = DummyVecEnv([lambda: SimpleTradingEnv(train_df, lookback=5)])\n",
        "        test_env = SimpleTradingEnv(test_df, lookback=5)\n",
        "\n",
        "        # Initialize PPO and A2C agents\n",
        "        ppo = PPO(\"MlpPolicy\", train_env, verbose=0)\n",
        "        a2c = A2C(\"MlpPolicy\", train_env, verbose=0)\n",
        "\n",
        "        # Train both models\n",
        "        ppo.learn(total_timesteps=50000)\n",
        "        a2c.learn(total_timesteps=50000)\n",
        "\n",
        "        # Function to evaluate an agent on the test environment\n",
        "        def eval_agent(model, env):\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action, _ = model.predict(obs)\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "            final_price = env.df.iloc[env.current_step - 1][\"Close\"]\n",
        "            final_value = env.cash + env.stock_owned * final_price\n",
        "            return final_value\n",
        "\n",
        "\n",
        "        # Evaluate both trained agents\n",
        "        ppo_val = eval_agent(ppo, test_env)\n",
        "        a2c_val = eval_agent(a2c, test_env)\n",
        "\n",
        "        # Simulate Buy & Hold strategy: buy 10 shares and hold till end\n",
        "        buy_hold_cash = 10000\n",
        "        buy_hold_shares = 10\n",
        "        start_price = test_df[\"Close\"].iloc[0]\n",
        "        end_price = test_df[\"Close\"].iloc[-1]\n",
        "\n",
        "        # Calculate Buy & Hold final value/Buy 10 shares if affordable\n",
        "        total_cost = buy_hold_shares * start_price\n",
        "        cash_left = buy_hold_cash - total_cost if buy_hold_cash >= total_cost else 0\n",
        "        buy_hold_val = cash_left + buy_hold_shares * end_price\n",
        "\n",
        "        # Calculate price change during test period\n",
        "        price_change = test_df[\"Close\"].iloc[-1] - test_df[\"Close\"].iloc[0]\n",
        "\n",
        "        # Store results for this ticker\n",
        "        results[ticker] = {\n",
        "            \"PPO_Absolute\": round(ppo_val, 2),\n",
        "            \"PPO_Profit\": round(ppo_val - 10000, 2),\n",
        "            \"PPO_ROI\": round(((ppo_val - 10000) / 10000) * 100, 2),\n",
        "\n",
        "            \"A2C_Absolute\": round(a2c_val, 2),\n",
        "            \"A2C_Profit\": round(a2c_val - 10000, 2),\n",
        "            \"A2C_ROI\": round(((a2c_val - 10000) / 10000) * 100, 2),\n",
        "\n",
        "            \"BuyHold_Absolute\": round(buy_hold_val, 2),\n",
        "            \"BuyHold_Profit\": round(buy_hold_val - 10000, 2),\n",
        "            \"BuyHold_ROI\": round(((buy_hold_val - 10000) / 10000) * 100, 2),\n",
        "\n",
        "            \"Price_Change\": round(price_change, 2),\n",
        "        }\n",
        "\n",
        "        # Print results to console\n",
        "        print(f\"‚úÖ {ticker} ‚Üí PPO: {results[ticker]['PPO_Absolute']}, A2C: {results[ticker]['A2C_Absolute']}, Buy & Hold: {results[ticker]['BuyHold_Absolute']}, Price Change: {results[ticker]['Price_Change']}\")\n",
        "        # print(f\"‚úÖ {ticker} ‚Üí PPO: {results[ticker]['PPO_Absolute']}, A2C: {results[ticker]['A2C_Absolute']}, Buy & Hold: {results[ticker]['BuyHold_Absolute']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If anything fails for a stock, skip and log the error\n",
        "        print(f\"‚ö†Ô∏è Skipped {ticker} due to error: {e}\")\n",
        "\n",
        "\n",
        "# ‚úÖ SAVE RESULTS TO CSV\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "results_df.index.name = \"Ticker\"\n",
        "\n",
        "# Save to drive folder\n",
        "results_df.to_csv(\"/content/drive/MyDrive/trading_results.csv\")\n",
        "print(\"üìÑ Results saved to Google Drive ‚Üí trading_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Inspect policy decisions\n",
        "Tests how each trained model responds to selected input states to verify policy behavior"
      ],
      "metadata": {
        "id": "0K5y33IMDnXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAlHgqtmJgj0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# # Policy Response Inspection Block\n",
        "\n",
        "# def test_policy_response(model, model_name, sample_states):\n",
        "#     print(f\"\\nüîç Policy inspection for {model_name}:\")\n",
        "#     for i, state in enumerate(sample_states):\n",
        "#         action, _ = model.predict(np.array(state).reshape(1, -1))\n",
        "#         print(f\"Input {i+1}: {state} ‚Üí Action: {action[0]}\")\n",
        "\n",
        "\n",
        "# sample_states = [\n",
        "#     [0] * ((4+2)*5 + 2),                                # all zeros, adjusted for new state size\n",
        "#     [100] * ((4)*5) + [10000]*5 + [100]*5 + [10000, 100],  # just example big values with indicators and holdings (adjust if needed)\n",
        "#     [1] * ((4)*5) + [1]*5 + [1]*5 + [10000, 100],           # flat prices + SMA + holdings\n",
        "#     test_env.reset(),                                      # real current market state\n",
        "# ]\n",
        "\n",
        "# test_policy_response(ppo, \"PPO\", sample_states)\n",
        "# test_policy_response(a2c, \"A2C\", sample_states)\n",
        "# # test_policy_response(ddpg, \"DDPG\", sample_states)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual: Plot ROI comparison:\n",
        "Plots bar chart comparing ROI across all strategies per stock."
      ],
      "metadata": {
        "id": "q8QfssVODvtq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E7239_TP6eDM"
      },
      "outputs": [],
      "source": [
        "# Transpose and sort results DataFrame by ticker\n",
        "results_df = pd.DataFrame(results).T.sort_index()\n",
        "\n",
        "# Plot ROI comparison for PPO, A2C, and Buy & Hold\n",
        "results_df[[\"PPO_ROI\", \"A2C_ROI\", \"BuyHold_ROI\"]].plot(kind=\"bar\", figsize=(18,6), alpha=0.85)\n",
        "\n",
        "# Add plot title and labels\n",
        "plt.title(\"Return on Investment (ROI) by Algorithm on Dow 30 Stocks (%)\")\n",
        "plt.ylabel(\"ROI (%)\")\n",
        "plt.xlabel(\"Ticker\")\n",
        "plt.xticks(rotation=90) # rotate x-axis labels for better readability\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual: Plot portfolio value comparison:\n",
        "Plots the final portfolio values per strategy for each stock."
      ],
      "metadata": {
        "id": "7MPnYFBdD9IH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_vn7iy3oFAk"
      },
      "outputs": [],
      "source": [
        "# Plot final portfolio values for PPO, A2C, and Buy & Hold\n",
        "results_df[[\"PPO_Absolute\", \"A2C_Absolute\", \"BuyHold_Absolute\"]].plot(kind=\"bar\", figsize=(18,6), alpha=0.85)\n",
        "\n",
        "# Add plot title and axis labels\n",
        "plt.title(\"Final Portfolio Value vs Buy & Hold on Dow 30 Stocks\")\n",
        "plt.ylabel(\"Portfolio Value ($)\")\n",
        "plt.xlabel(\"Ticker\")\n",
        "plt.xticks(rotation=90) # Rotate ticker labels for better readability\n",
        "\n",
        "# Show grid for visual clarity\n",
        "plt.grid(True)\n",
        "\n",
        "# Draw a red dashed line at the initial capital of $10,000\n",
        "plt.axhline(10000, color='red', linestyle='--', linewidth=1.5, label='Initial Capital ($10,000)')\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual: Cumulative reward curves\n",
        "Plots cumulative reward during evaluation to visualize how the agent‚Äôs portfolio grows over time.\n",
        "\n",
        "Not used in thesis"
      ],
      "metadata": {
        "id": "1i0__BP0EDzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIunVFYlV_iO"
      },
      "outputs": [],
      "source": [
        "# Function to plot cumulative reward for a given model and environment\n",
        "def plot_model_reward(model, env, label):\n",
        "    obs = env.reset()\n",
        "    rewards = []\n",
        "\n",
        "    # Simulate one full episode\n",
        "    for _ in range(len(env.df) - 1):\n",
        "        action, _ = model.predict(obs) # get model's action\n",
        "        obs, reward, done, _ = env.step(action) # apply action and receive reward\n",
        "        # Ensure reward is a scalar float (handles cases where reward is a list/array)\n",
        "        rewards.append(float(reward[0]) if isinstance(reward, (list, np.ndarray)) else float(reward))\n",
        "        if done:\n",
        "            break\n",
        "    # Plot cumulative sum of rewards\n",
        "    plt.plot(np.cumsum(rewards), label=label)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot cumulative rewards for both PPO and A2C\n",
        "plot_model_reward(ppo, test_env, \"PPO\")\n",
        "plot_model_reward(a2c, test_env, \"A2C\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Cumulative Reward Curve on AAPL (Test Set)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Cumulative Reward ($)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H3XC83q1DCm"
      },
      "source": [
        "# Optional: Check what action is taken and what reward is given at each step\n",
        "\n",
        "Has to be adjusted for different algorithm PPO vs A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMb4DlL73jNf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "ppo_val = 0\n",
        "obs = test_env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "# Step through the environment using the PPO agent\n",
        "while not done:\n",
        "    action, _ = ppo.predict(obs)  # Predict next action using PPO model\n",
        "    print(f\"Step {step}: Action = {action}\")  # Print current action\n",
        "    obs, reward, done, _ = test_env.step(action)  # Apply action and get feedback\n",
        "    print(f\"Reward = {reward}\")  # Print reward for the step\n",
        "\n",
        "    # Convert reward to scalar if it's in array/list format\n",
        "    reward_val = reward[0] if isinstance(reward, (list, np.ndarray)) else reward\n",
        "    ppo_val += reward_val  # Accumulate total reward\n",
        "    step += 1\n",
        "\n",
        "# Print total cumulative reward as a proxy for portfolio value\n",
        "print(f\"üí∞ Final PPO Portfolio Value: {round(ppo_val, 2)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}